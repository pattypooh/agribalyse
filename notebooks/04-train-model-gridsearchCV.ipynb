{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import  SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from src.data import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = preprocessing.dict_params\n",
    "INPUT_FILE_PATH = './../data/processed'\n",
    "OUTPUT_FILE_PATH = './../models'\n",
    "INDEX_KEY = [0] # the 1st column is the primary key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'synthese': {'file_name': 'Agribalyse_Synthese.csv', 'keep_cols': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'index_key': [0]}, 'ingredients': {'file_name': 'Agribalyse_Detail ingredient.csv', 'keep_cols': [0, 2, 3, 4, 5, 6, 7], 'index_key': [0], 'pivot_idx_key': [0, 1, 2, 3, 4], 'pivot_idx_col': 5, 'pivot_idx_values': 6, 'pivot_keep_cols': [0, 2, 3, 4, 5, 6, 7]}, 'etapes': {'file_name': 'Agribalyse_Detail etape.csv', 'keep_cols': [0, 8, 9, 10, 11, 12, 13], 'index_key': [0]}}\n"
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Abats de bœuf', 'Abats de porc', 'Abats de poulet', 'Abats de veau',\n",
       "       'Abricot', 'Ail', 'Amande', 'Amidon de maïs', 'Ananas', 'Anchois',\n",
       "       ...\n",
       "       'max_EF_Viande de moutton sans os', 'max_EF_Viande de porc maigre',\n",
       "       'max_EF_Viande de poulet sans os', 'max_EF_Viande de veau sans os',\n",
       "       'max_EF_Vin blanc', 'max_EF_Vin rouge', 'max_EF_Yaourt',\n",
       "       'max_EF_citron', 'max_EF_Échalote', 'max_EF_Œuf de poule'],\n",
       "      dtype='object', length=639)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_ori = pd.read_csv(os.path.join(INPUT_FILE_PATH, preprocessing.get_param('ingredients', 'file_name')))\n",
    "data_df_ori.columns[13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df)->pd.DataFrame:\n",
    "    # Drop columns\n",
    "    drop_cols = ['Code_AGB', 'Code_CIQUAL', 'Groupe_aliment', 'Sous-groupe_aliment', 'Nom_Produit_Francais', 'LCI_Name',\\\n",
    "          'Saisonnalite', 'Transport_par_avion_', 'Livraison', 'Livraison', 'Materiau_emballage', 'Preparation', 'DQR_Note_qualite_la_donnee_']\n",
    "    clean_df = df.drop(drop_cols, axis=1)\n",
    "\n",
    "    # Drop lines with no ingredients (all 0)\n",
    "    with_ing=clean_df.iloc[:,1:].sum(axis=1)!=0\n",
    "   \n",
    "    clean_df = clean_df[with_ing].copy()\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = clean_data(data_df_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ingredients kept for training :640\n",
      "Number of products kept for training:1038\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of ingredients kept for training :{len(data_df.columns)}')\n",
    "print(f'Number of products kept for training:{data_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df = data_df.dropna(axis=0)\n",
    "y= data_df.loc[:,'Score_unique_EF_'].copy()\n",
    "X= data_df.drop('Score_unique_EF_', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(830, 639)\n",
      "y_train:(830,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train:{X_train.shape}')\n",
    "print(f'y_train:{y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_train)\n",
    "    print(f'Train - R2 Score: {r2_score(y_train, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridsearchCV Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingred_features = X_train.columns.to_list()\n",
    "cat_features = []\n",
    "num_features = []\n",
    "\n",
    "svd_transformer = Pipeline([\n",
    "    ('svd', TruncatedSVD(n_components=32)),])\n",
    "\n",
    "# transformer for numerical features\n",
    "num_transformer = Pipeline([\n",
    "        ('imputer_num', SimpleImputer(strategy = 'median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "# transformer for categorical features\n",
    "cat_transformer = Pipeline([\n",
    "        ('imputer_cat', SimpleImputer(strategy = 'most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('categoricals', cat_transformer, cat_features),\n",
    "        ('numericals', num_transformer, num_features),\n",
    "        ('dim_reduction', svd_transformer, ingred_features),\n",
    "    ],\n",
    "    remainder = 'drop'\n",
    ")\n",
    "full_pipeline = Pipeline([\n",
    "        ('transformation', preprocessor),\n",
    "        ('regressor', RandomForestRegressor())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done.\n",
      "Best hyperparameters :  {'regressor__max_depth': 20, 'regressor__max_features': 'auto', 'regressor__n_estimators': 100}\n",
      "Best R2 score :  0.60641260059794\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    #'transformation__n_components': [37],\n",
    "    'regressor__max_depth': [10, 15, 20, 500] ,\n",
    "    'regressor__n_estimators': [10, 50, 100, 500],\n",
    "    #'min_samples_split': [],\n",
    "    'regressor__max_features': ['auto', 'log2']\n",
    "}\n",
    "gridsearch = GridSearchCV(full_pipeline, param_grid = params, cv = 3) # cv : the number of folds to be used for CV\n",
    "gridsearch.fit(X_train, y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best R2 score : \", gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores on train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - R2 Score: 0.9023083357920088\n",
      "0.8030169311511185\n"
     ]
    }
   ],
   "source": [
    "best_predictor = gridsearch.best_estimator_\n",
    "fit_predict(best_predictor,X_train, y_train)\n",
    "\n",
    "score_testset = r2_score(y_test, best_predictor.predict(X_test))\n",
    "print(score_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done.\n",
      "Best hyperparameters :  {'regressor__max_depth': 10, 'regressor__max_features': 'log2', 'regressor__min_samples_split': 6, 'regressor__n_estimators': 50}\n",
      "Best R2 score :  0.6378853853654733\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    #'transformation__n_components': [37],\n",
    "    'regressor__max_depth': [10, 12] , #1st try[5, 10, 12, 18, 20]->Score: 0.68\n",
    "    'regressor__n_estimators': [10, 15, 50, 100],\n",
    "    'regressor__min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'regressor__max_features': ['auto', 'log2']\n",
    "}\n",
    "gridsearch = GridSearchCV(full_pipeline, param_grid = params, cv = 5) # 1st try: cv=5 the number of folds to be used for CV\n",
    "gridsearch.fit(X_train, y_train)\n",
    "print(\"...Done.\")\n",
    "print(\"Best hyperparameters : \", gridsearch.best_params_)\n",
    "print(\"Best R2 score : \", gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores on train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - R2 Score: 0.847825547037343\n",
      "0.7401983238094778\n"
     ]
    }
   ],
   "source": [
    "best_predictor = gridsearch.best_estimator_\n",
    "fit_predict(best_predictor,X_train, y_train)\n",
    "\n",
    "score_testset = r2_score(y_test, best_predictor.predict(X_test))\n",
    "print(score_testset)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5cc47c8831822e32dd5f765150260df6f896d34b43f3e216997ee9b0d9ca468e"
  },
  "kernelspec": {
   "display_name": "kernel-pez-jedha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
